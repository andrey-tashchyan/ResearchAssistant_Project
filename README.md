# Pipeline PSID - Traitement et Construction de Panels de DonnÃ©es

## ğŸ“‹ Table des matiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [Architecture du projet](#architecture-du-projet)
3. [Installation et prÃ©requis](#installation-et-prÃ©requis)
4. [Structure des donnÃ©es](#structure-des-donnÃ©es)
5. [Pipeline complet](#pipeline-complet)
6. [Scripts dÃ©taillÃ©s](#scripts-dÃ©taillÃ©s)
7. [Fichiers de configuration](#fichiers-de-configuration)
8. [Utilisation](#utilisation)
9. [Formats de sortie](#formats-de-sortie)
10. [Optimisations mÃ©moire](#optimisations-mÃ©moire)
11. [DÃ©pannage](#dÃ©pannage)
12. [Exemples d'utilisation](#exemples-dutilisation)

---

## ğŸ¯ Vue d'ensemble

Ce projet constitue un **pipeline complet et optimisÃ©** pour le traitement des donnÃ©es du **Panel Study of Income Dynamics (PSID)**. Il transforme les fichiers bruts SAS/TXT en panels structurÃ©s et exploitables, avec un accent particulier sur:

- **L'efficacitÃ© mÃ©moire** : gestion de datasets de plusieurs dizaines de millions de lignes
- **La traÃ§abilitÃ©** : logging dÃ©taillÃ© de chaque Ã©tape
- **La flexibilitÃ©** : configuration via fichiers externes
- **La performance** : sortie en format Parquet partitionnÃ© avec compression ZSTD

### Objectifs principaux

1. **Conversion** : Transformer les fichiers SAS/TXT en CSV exploitables
2. **Mapping** : CrÃ©er une correspondance canonique entre variables et annÃ©es
3. **Grille canonique** : Construire une matrice variables Ã— annÃ©es
4. **Fusion** : Combiner des variables similaires selon des rÃ¨gles dÃ©finies
5. **Panel final** : GÃ©nÃ©rer des panels longs optimisÃ©s par famille et par annÃ©e

---

## ğŸ—ï¸ Architecture du projet

```
algo3/
â”‚
â”œâ”€â”€ ğŸ“‚ sorted_data/              # DonnÃ©es sources PSID
â”‚   â”œâ”€â”€ FAM2009ER.sas           # Scripts SAS de dÃ©finition
â”‚   â”œâ”€â”€ FAM2009ER.txt           # DonnÃ©es brutes (format fixe)
â”‚   â”œâ”€â”€ FAM2009ER_full.csv      # CSV converti (gÃ©nÃ©rÃ©)
â”‚   â”œâ”€â”€ WLTH1999.sas
â”‚   â”œâ”€â”€ WLTH1999.txt
â”‚   â””â”€â”€ ...                      # Autres annÃ©es (1999-2023)
â”‚
â”œâ”€â”€ ğŸ“‚ out/                      # RÃ©sultats intermÃ©diaires
â”‚   â”œâ”€â”€ mapping_long.csv         # Dictionnaire complet des variables
â”‚   â”œâ”€â”€ fam_wlth_inventory.csv  # Inventaire des modules FAM/WLTH
â”‚   â”œâ”€â”€ canonical_grid.csv       # Grille canonique initiale
â”‚   â”œâ”€â”€ canonical_grid_merged.csv # Grille aprÃ¨s fusion de lignes
â”‚   â””â”€â”€ final_grid.csv           # Grille finale utilisÃ©e pour l'extraction
â”‚
â”œâ”€â”€ ğŸ“‚ final_results/            # RÃ©sultats finaux
â”‚   â”œâ”€â”€ panel_parent_child.csv   # Panel long individuel
â”‚   â”œâ”€â”€ parent_child_links.csv   # Relations parent-enfant
â”‚   â”œâ”€â”€ panel_summary.csv        # Statistiques descriptives
â”‚   â”œâ”€â”€ codes_resolved_audit.csv # Journal de correspondances
â”‚   â”œâ”€â”€ panel_grid_by_family.csv # â˜… Panel par famille (format wide)
â”‚   â”œâ”€â”€ panel.parquet/           # â˜… Panel optimisÃ© (format partitionnÃ©)
â”‚   â””â”€â”€ family_grids/            # (Optionnel) Un CSV par famille
â”‚
â”œâ”€â”€ ğŸ“œ Scripts Python principaux
â”‚   â”œâ”€â”€ sas_to_csv.py            # [1] Conversion SAS/TXT â†’ CSV
â”‚   â”œâ”€â”€ create_mapping.py        # [2] Construction du mapping
â”‚   â”œâ”€â”€ psid_tool.py             # [3] Grille canonique
â”‚   â”œâ”€â”€ merge_grid.py            # [4] Fusion de lignes
â”‚   â”œâ”€â”€ build_final_panel.py     # [5] Panel optimisÃ© Parquet
â”‚   â””â”€â”€ build_panel_parent_child.py # Panel famille-enfant
â”‚
â”œâ”€â”€ ğŸ“œ Scripts utilitaires
â”‚   â”œâ”€â”€ filter_grid_rows.py      # Filtrage de la grille
â”‚   â”œâ”€â”€ make_canonical_grid.py   # Alternative pour grille canonique
â”‚   â”œâ”€â”€ no_children_from_gid.py  # Analyse GID sans enfants
â”‚   â”œâ”€â”€ sas_to_csv_gid.py        # Conversion GID spÃ©cifique
â”‚   â””â”€â”€ build_parent_child_presence_matrix.py # Matrice de prÃ©sence
â”‚
â”œâ”€â”€ ğŸ“œ Fichiers de configuration
â”‚   â”œâ”€â”€ file_list.txt            # Liste des fichiers Ã  convertir
â”‚   â””â”€â”€ merge_groups.txt         # RÃ¨gles de fusion de variables
â”‚
â”œâ”€â”€ ğŸ“œ Orchestration
â”‚   â”œâ”€â”€ run_all.sh               # â˜… Script maÃ®tre exÃ©cutant tout le pipeline
â”‚   â””â”€â”€ README.md                # Ce fichier
â”‚
â””â”€â”€ ğŸ“‚ .vscode/                  # Configuration VSCode
    â””â”€â”€ extensions.json
```

---

## ğŸ’» Installation et prÃ©requis

### PrÃ©requis systÃ¨me

- **Python 3.8+** (testÃ© avec 3.9 et 3.10)
- **8 GB RAM minimum** (16 GB recommandÃ© pour les gros datasets)
- **10 GB d'espace disque** pour les fichiers intermÃ©diaires et finaux

### DÃ©pendances Python

```bash
# DÃ©pendances principales
pip install pandas>=1.5.0
pip install numpy>=1.23.0
pip install pyarrow>=10.0.0  # Pour le format Parquet

# Optionnel mais recommandÃ©
pip install tqdm              # Barres de progression
pip install fastparquet       # Alternative Ã  pyarrow
```

### Installation complÃ¨te avec environnement virtuel

```bash
# CrÃ©er un environnement virtuel
python3 -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate

# Installer les dÃ©pendances
pip install --upgrade pip
pip install pandas numpy pyarrow tqdm

# VÃ©rifier l'installation
python -c "import pandas; import pyarrow; print('OK')"
```

---

## ğŸ“Š Structure des donnÃ©es

### Sources PSID

Le projet traite deux types principaux de modules PSID:

#### 1. **Module FAM (Family)** - DonnÃ©es dÃ©mographiques et familiales
- Variables familiales (taille du mÃ©nage, composition)
- CaractÃ©ristiques du chef de famille
- Informations sur les enfants
- Format: `FAM{YEAR}ER.txt` et `FAM{YEAR}ER.sas`
- AnnÃ©es disponibles: 2009, 2011, 2013, 2015, 2017, 2019, 2021, 2023

#### 2. **Module WLTH (Wealth)** - DonnÃ©es patrimoniales
- Actifs (immobilier, actions, Ã©pargne)
- Dettes (hypothÃ¨ques, prÃªts)
- IRA et comptes de retraite
- Format: `WLTH{YEAR}.txt` et `WLTH{YEAR}.sas`
- AnnÃ©es disponibles: 1999, 2001, 2003, 2005, 2007, 2009, 2011, 2013, ...

### Format des fichiers sources

**Fichiers SAS (.sas)**
- Scripts de lecture dÃ©finissant les positions et largeurs des colonnes
- Contiennent les mÃ©tadonnÃ©es (noms de variables, types, labels)

**Fichiers TXT (.txt)**
- DonnÃ©es brutes en format largeur fixe (Fixed-Width Format)
- Pas de sÃ©parateurs, positions dÃ©finies par le fichier .sas
- Exemple: une ligne = un enregistrement, chaque variable Ã  position fixe

---

## ğŸ”„ Pipeline complet

Le pipeline s'exÃ©cute en **5 Ã©tapes sÃ©quentielles** via le script `run_all.sh`:

```
[1] SAS/TXT â†’ CSV  â†’  [2] Mapping  â†’  [3] Grille canonique  â†’  [4] Fusion  â†’  [5] Panel final
```

### Diagramme de flux

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  file_list.txt  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [1] sas_to_csv.py               â”‚
â”‚ Input:  FAM*.sas, FAM*.txt      â”‚
â”‚         WLTH*.sas, WLTH*.txt    â”‚
â”‚ Output: *_full.csv              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [2] create_mapping.py           â”‚
â”‚ Input:  sorted_data/*.csv       â”‚
â”‚ Output: mapping_long.csv        â”‚
â”‚         fam_wlth_inventory.csv  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [3] psid_tool.py                â”‚
â”‚ Input:  mapping_long.csv        â”‚
â”‚ Output: canonical_grid.csv      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [4] merge_grid.py               â”‚
â”‚ Input:  canonical_grid.csv      â”‚
â”‚         merge_groups.txt        â”‚
â”‚ Output: canonical_grid_merged   â”‚
â”‚         â†’ final_grid.csv        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [5] build_final_panel.py        â”‚
â”‚ Input:  final_grid.csv          â”‚
â”‚         mapping_long.csv        â”‚
â”‚         sorted_data/*.csv       â”‚
â”‚ Output: panel.parquet/          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Scripts dÃ©taillÃ©s

### 1ï¸âƒ£ sas_to_csv.py - Conversion SAS/TXT vers CSV

**Objectif**: Convertir les fichiers bruts PSID (TXT Ã  largeur fixe) en CSV exploitables.

#### FonctionnalitÃ©s
- Parse les scripts SAS (.sas) pour extraire les mÃ©tadonnÃ©es de colonnes
- Lit les fichiers TXT avec `pd.read_fwf()` (fixed-width format)
- Ajoute une ligne de labels (2e ligne du CSV)
- Gestion des erreurs de parsing avec fallback
- Barres de progression (si tqdm installÃ©)

#### Utilisation
```bash
# Mode manuel
python sas_to_csv.py \
  --file-list file_list.txt \
  --out-dir sorted_data

# Via run_all.sh
./run_all.sh  # Ã‰tape 1 automatique
```

#### EntrÃ©es
- `file_list.txt`: Liste des fichiers Ã  convertir
  ```
  FAM2009ER.sas
  FAM2009ER.txt
  WLTH1999.sas
  WLTH1999.txt
  ```

#### Sorties
- `sorted_data/FAM{YEAR}ER_full.csv`
- `sorted_data/WLTH{YEAR}_full.csv`
- Structure:
  - Ligne 1: Noms de variables (codes)
  - Ligne 2: Labels descriptifs
  - Lignes suivantes: DonnÃ©es

#### Performance
- ~1-2 minutes par fichier (variable selon taille)
- Logging colorisÃ© avec temps d'exÃ©cution

---

### 2ï¸âƒ£ create_mapping.py - Construction du dictionnaire de variables

**Objectif**: CrÃ©er une table de correspondance exhaustive entre variables PSID et leur dÃ©finition.

#### FonctionnalitÃ©s
- Analyse les headers de tous les CSV gÃ©nÃ©rÃ©s
- Extrait les labels de la 2e ligne
- Normalise les noms de variables en concepts canoniques
- DÃ©tecte automatiquement le type (FAM/WLTH)
- Applique des rÃ¨gles de synonymes et normalisation
- Devine les types de donnÃ©es (numeric/string)

#### Utilisation
```bash
python create_mapping.py \
  --data-dir sorted_data \
  --out-dir out
```

#### Sorties principales

**mapping_long.csv** - Format long exhaustif
```csv
canonical,year,file_type,var_code,label,category,dtype,required,transform
family_id,2009,FAM,ER42001,"Family ID",Demographics,string,1,
num_children,2009,FAM,ER42003,"Number of children",Demographics,int,1,
ira_balance,1999,WLTH,S517,"IRA Balance",Retirement/IRA,float,0,
```

Colonnes:
- `canonical`: Nom de concept normalisÃ©
- `year`: AnnÃ©e de l'enquÃªte
- `file_type`: FAM ou WLTH
- `var_code`: Code variable PSID original
- `label`: Description textuelle
- `category`: CatÃ©gorie thÃ©matique
- `dtype`: Type de donnÃ©es infÃ©rÃ©
- `required`: 1 si variable indispensable, 0 sinon
- `transform`: Transformation Ã©ventuelle Ã  appliquer

**fam_wlth_inventory.csv** - Inventaire des modules
```csv
year,module,file_path,num_variables,num_rows
2009,FAM,sorted_data/FAM2009ER_full.csv,723,9144
1999,WLTH,sorted_data/WLTH1999_full.csv,412,7406
```

---

### 3ï¸âƒ£ psid_tool.py - Grille canonique (Variables Ã— AnnÃ©es)

**Objectif**: CrÃ©er une matrice pivotÃ©e avec les variables en lignes et les annÃ©es en colonnes.

#### FonctionnalitÃ©s
- Pivote `mapping_long.csv` en format wide
- RÃ©sout les conflits de variables (prÃ©fÃ©rence WLTH par dÃ©faut)
- Ajoute une colonne `row` (numÃ©rotation 1-based)
- Filtre optionnel par annÃ©es
- GÃ¨re les doublons et variables manquantes

#### Utilisation
```bash
python psid_tool.py \
  --mapping out/mapping_long.csv \
  --out-dir out \
  --prefer WLTH           # PrÃ©fÃ©rer WLTH en cas de conflit
  --years 1999,2001,2009  # Optionnel: filtrer par annÃ©es
```

#### Sortie: canonical_grid.csv

Format:
```csv
row,concept,required,1999,2001,2003,2005,2007,2009,...,2023
1,family_id,1,ER30001,ER31001,ER32001,...,ER42001
2,person_id,1,ER30002,ER31002,ER32002,...,ER42002
3,ira_balance,0,S517,S617,S717,S817,,ER46946
```

Colonnes:
- `row`: NumÃ©ro de ligne (ordre canonique)
- `concept`: Nom du concept normalisÃ©
- `required`: Indicateur de variable indispensable
- `{YEAR}`: Code variable pour chaque annÃ©e (vide si absent)

---

### 4ï¸âƒ£ merge_grid.py - Fusion de lignes similaires

**Objectif**: Combiner plusieurs lignes de variables similaires en une seule ligne consolidÃ©e.

#### FonctionnalitÃ©s
- Lit les rÃ¨gles de fusion depuis `merge_groups.txt`
- Fusionne les codes de variables par prioritÃ© (gauche â†’ droite)
- Conserve la premiÃ¨re valeur non vide par annÃ©e
- Ajoute un suffixe `_merged` aux concepts fusionnÃ©s

#### Utilisation
```bash
python merge_grid.py \
  --file out/canonical_grid.csv \
  --out out/canonical_grid_merged.csv \
  < merge_groups.txt

# Ou via stdin
cat merge_groups.txt | python merge_grid.py \
  --file out/canonical_grid.csv \
  --out out/canonical_grid_merged.csv
```

#### Format merge_groups.txt

Chaque ligne dÃ©finit un groupe de fusion:
```
ira_balance ira_any ira_num
wealth_wo_equity home_equity
vehicles vehicle
```

RÃ¨gles:
- SÃ©parer les concepts par espaces
- Le premier concept devient le nom fusionnÃ© (+ `_merged`)
- Pour chaque annÃ©e, prend la premiÃ¨re valeur non vide de gauche Ã  droite
- Les lignes originales sont supprimÃ©es, une seule ligne fusionnÃ©e crÃ©Ã©e

#### Exemple

**Avant fusion:**
```csv
concept,1999,2001,2003
ira_balance,S517,,S717
ira_any,,S618,
```

**RÃ¨gle:** `ira_balance ira_any`

**AprÃ¨s fusion:**
```csv
concept,1999,2001,2003
ira_balance_merged,S517,S618,S717
```

#### Sortie
- `canonical_grid_merged.csv`
- CopiÃ© automatiquement vers `final_grid.csv` par `run_all.sh`

---

### 5ï¸âƒ£ build_final_panel.py - Panel optimisÃ© en Parquet

**Objectif**: GÃ©nÃ©rer le panel final en format Parquet partitionnÃ© pour une analyse efficace.

#### FonctionnalitÃ©s clÃ©s
- Lecture optimisÃ©e chunk par chunk
- Downcast automatique des types (Int32, float32, category)
- Compression ZSTD
- Partitionnement par annÃ©e (et optionnellement par module)
- Logging dÃ©taillÃ© avec temps d'exÃ©cution
- Support de `--rebuild` pour reconstruire depuis zÃ©ro

#### Utilisation
```bash
python build_final_panel.py \
  --data-dir sorted_data \
  --out-dir out \
  --final-dir final_results \
  --rebuild                    # Optionnel: effacer et reconstruire
  --partition-by-module        # Optionnel: partitionner aussi par FAM/WLTH
```

#### Processus interne
1. **DÃ©couverte** : Scan `sorted_data/` pour fichiers `*_full.csv`
2. **Mapping** : Construit des mappings par (annÃ©e, module)
3. **Traitement chunk** :
   - Pour chaque (annÃ©e, module):
     - Lit seulement les colonnes nÃ©cessaires
     - Renomme selon mapping
     - Downcast des types
     - Ã‰crit en Parquet partitionnÃ©
4. **Manifest** : GÃ©nÃ¨re des statistiques de sortie

#### Optimisations mÃ©moire
- **Copy-on-write** : `pd.options.mode.copy_on_write = True`
- **Downcasting agressif** :
  - int â†’ Int32 (nullable)
  - float â†’ float32
  - string rÃ©pÃ©titifs â†’ category
- **Garbage collection** : `gc.collect()` aprÃ¨s chaque chunk
- **Streaming** : Un seul chunk en mÃ©moire Ã  la fois

#### Sortie: panel.parquet/

Structure du rÃ©pertoire Parquet partitionnÃ©:
```
final_results/panel.parquet/
â”œâ”€â”€ year=1999/
â”‚   â”œâ”€â”€ module=FAM/
â”‚   â”‚   â””â”€â”€ part-0.parquet
â”‚   â””â”€â”€ module=WLTH/
â”‚       â””â”€â”€ part-0.parquet
â”œâ”€â”€ year=2001/
â”‚   â”œâ”€â”€ module=FAM/
â”‚   â”‚   â””â”€â”€ part-0.parquet
â”‚   â””â”€â”€ module=WLTH/
â”‚       â””â”€â”€ part-0.parquet
...
â””â”€â”€ _common_metadata
```

Format des colonnes:
- `year` (int32): AnnÃ©e d'enquÃªte
- `module` (category): FAM ou WLTH
- Variables canoniques (types optimisÃ©s)

#### Performance
- 10-50x plus rapide que CSV Ã  la lecture
- Compression ~70-80% par rapport Ã  CSV
- RequÃªtes filtrÃ©es ultra-rapides via prÃ©dicats Parquet

---

### 6ï¸âƒ£ build_panel_parent_child.py - Panel parent-enfant

**Objectif**: Construire un panel centrÃ© sur les relations familiales (parent â†’ enfant).

#### FonctionnalitÃ©s
- Extrait les variables "required" de `final_grid.csv`
- Identifie les liens parent-enfant via `mother_id`, `father_id`
- Filtre pour ne garder que les familles avec enfants
- GÃ©nÃ¨re plusieurs vues complÃ©mentaires
- Format wide par famille (variables Ã— annÃ©es)

#### Utilisation
```bash
python build_panel_parent_child.py \
  --final-grid out/final_grid.csv \
  --mapping out/mapping_long.csv \
  --data-dir sorted_data \
  --out-dir final_results \
  --prefer WLTH                       # PrÃ©fÃ©rer WLTH en cas de conflit
  --write-family-files                # Optionnel: 1 CSV par famille
```

#### Sorties dans final_results/

**1. panel_parent_child.csv** - Panel long individuel
```csv
year,family_id,person_id,mother_id,father_id,concept1,concept2,...
2009,100001,101,102,103,value1,value2,...
2009,100001,102,,,value1,value2,...
2009,100001,103,,,value1,value2,...
2011,100001,101,102,103,value1,value2,...
```

**2. parent_child_links.csv** - Relations parent-enfant
```csv
year,family_id,person_id,mother_id,father_id,is_parent
2009,100001,101,102,103,False
2009,100001,102,,,True
2009,100001,103,,,True
```

**3. panel_summary.csv** - Statistiques descriptives
```csv
concept,non_missing,mean,median,std
ira_balance,12450,45678.32,28000.0,51234.12
num_children,24850,2.3,2.0,1.2
```

**4. codes_resolved_audit.csv** - Journal de correspondances
```csv
concept,year,var_code,file_type
family_id,2009,ER42001,FAM
ira_balance,1999,S517,WLTH
```

**5. â˜… panel_grid_by_family.csv** - Format wide par famille
```csv
family_id,concept,1999,2001,2003,2005,2007,2009,...
100001,family_id,100001,100001,100001,100001,100001,100001,...
100001,num_children,2,2,3,3,3,2,...
100001,ira_balance,15000,18000,22000,28000,35000,42000,...
100002,family_id,100002,100002,100002,100002,100002,100002,...
100002,num_children,1,1,1,2,2,2,...
```

Structure:
- Index multi-niveau conceptuel: (family_id, concept)
- Une ligne par (famille, concept)
- Colonnes = annÃ©es
- Valeurs = agrÃ©gation par famille (prioritÃ© aux parents)

**6. family_grids/{family_id}.csv** (optionnel avec --write-family-files)

Un fichier CSV par famille:
```
family_grids/
â”œâ”€â”€ 100001.csv
â”œâ”€â”€ 100002.csv
â”œâ”€â”€ 100003.csv
...
```

Chaque fichier contient la grille de cette famille uniquement:
```csv
concept,1999,2001,2003,2005,...
family_id,100001,100001,100001,100001,...
num_children,2,2,3,3,...
ira_balance,15000,18000,22000,28000,...
```

#### RÃ¨gles d'agrÃ©gation

Pour chaque (famille, annÃ©e, concept):
1. Si variable identifiable chez un parent â†’ prendre valeur parent
2. Sinon, prendre la premiÃ¨re valeur non manquante de n'importe quel membre
3. Si aucune valeur disponible â†’ `pd.NA`

---

## ğŸ“„ Fichiers de configuration

### file_list.txt

Liste des paires SAS/TXT Ã  convertir en CSV.

**Format**:
```
FAM2009ER.sas
FAM2009ER.txt
FAM2011ER.sas
FAM2011ER.txt
WLTH1999.sas
WLTH1999.txt
...
```

**RÃ¨gles**:
- Une ligne par fichier
- Toujours par paire: `.sas` puis `.txt`
- Noms relatifs ou absolus
- Chemins rÃ©solus depuis `sorted_data/`

### merge_groups.txt

DÃ©finit les groupes de variables Ã  fusionner.

**Format**:
```
concept1 concept2 concept3
concept4 concept5
```

**RÃ¨gles**:
- Une ligne = un groupe de fusion
- Concepts sÃ©parÃ©s par espaces
- Premier concept = nom de base (+ `_merged`)
- Fusion par prioritÃ© gauche â†’ droite

**Exemple rÃ©aliste**:
```
ira_balance ira_any ira_num ira_contrib
wealth_wo_equity home_equity other_assets
mortgage debt vehicle_loan
```

---

## ğŸš€ Utilisation

### ExÃ©cution complÃ¨te du pipeline

La mÃ©thode **recommandÃ©e** est d'utiliser le script maÃ®tre:

```bash
# Rendre le script exÃ©cutable (une seule fois)
chmod +x run_all.sh

# Lancer le pipeline complet
./run_all.sh
```

Le script:
1. VÃ©rifie la prÃ©sence des rÃ©pertoires
2. ExÃ©cute les 5 Ã©tapes dans l'ordre
3. Affiche des logs colorisÃ©s avec timestamps
4. S'arrÃªte en cas d'erreur
5. Affiche un rÃ©sumÃ© final avec temps d'exÃ©cution total

### Mode verbeux / silencieux

```bash
# Mode silencieux (erreurs uniquement)
QUIET=1 ./run_all.sh

# Mode trÃ¨s verbeux
VERBOSE=1 ./run_all.sh

# Combinaison
VERBOSE=0 QUIET=1 ./run_all.sh
```

### ExÃ©cution partielle

Si vous voulez relancer seulement certaines Ã©tapes:

```bash
# Ã‰tape 1 uniquement (conversion)
python sas_to_csv.py --file-list file_list.txt --out-dir sorted_data

# Ã‰tape 2 uniquement (mapping)
python create_mapping.py --data-dir sorted_data --out-dir out

# Ã‰tape 3 uniquement (grille)
python psid_tool.py --mapping out/mapping_long.csv --out-dir out --prefer WLTH

# Ã‰tape 4 uniquement (fusion)
python merge_grid.py --file out/canonical_grid.csv --out out/canonical_grid_merged.csv < merge_groups.txt
cp out/canonical_grid_merged.csv out/final_grid.csv

# Ã‰tape 5 uniquement (panel final)
python build_final_panel.py --data-dir sorted_data --out-dir out --final-dir final_results --rebuild
```

### Mode rebuild (reconstruction complÃ¨te)

Pour forcer une reconstruction depuis zÃ©ro:

```bash
# Nettoyer tous les fichiers intermÃ©diaires
rm -rf out/* final_results/*

# Relancer le pipeline
./run_all.sh
```

Ou cibler seulement le panel final:

```bash
python build_final_panel.py \
  --data-dir sorted_data \
  --out-dir out \
  --final-dir final_results \
  --rebuild  # Force la suppression et recrÃ©ation de panel.parquet/
```

---

## ğŸ“¤ Formats de sortie

### 1. Panel Parquet (recommandÃ© pour analyse)

**Fichier**: `final_results/panel.parquet/`

**Lecture en Python**:

```python
import pandas as pd

# Lecture complÃ¨te (attention Ã  la mÃ©moire!)
df = pd.read_parquet('final_results/panel.parquet')

# Lecture d'une seule annÃ©e (trÃ¨s rapide grÃ¢ce au partitionnement)
df_2009 = pd.read_parquet(
    'final_results/panel.parquet',
    filters=[('year', '==', 2009)]
)

# Lecture de plusieurs annÃ©es
df_recent = pd.read_parquet(
    'final_results/panel.parquet',
    filters=[('year', 'in', [2015, 2017, 2019, 2021, 2023])]
)

# Lecture du module WLTH uniquement
df_wlth = pd.read_parquet(
    'final_results/panel.parquet',
    filters=[('module', '==', 'WLTH')]
)

# Lecture de colonnes spÃ©cifiques (trÃ¨s efficace)
df_subset = pd.read_parquet(
    'final_results/panel.parquet',
    columns=['year', 'family_id', 'ira_balance', 'num_children']
)

# Combinaison de filtres
df_filtered = pd.read_parquet(
    'final_results/panel.parquet',
    filters=[
        ('year', '>=', 2009),
        ('module', '==', 'WLTH')
    ],
    columns=['year', 'family_id', 'ira_balance']
)
```

**Lecture en R** (avec arrow):

```r
library(arrow)

# Lecture complÃ¨te
df <- read_parquet("final_results/panel.parquet")

# Lecture avec filtres
df_2009 <- open_dataset("final_results/panel.parquet") %>%
  filter(year == 2009) %>%
  collect()

# Lecture optimisÃ©e avec dplyr
df_filtered <- open_dataset("final_results/panel.parquet") %>%
  filter(year >= 2009, module == "WLTH") %>%
  select(year, family_id, ira_balance) %>%
  collect()
```

### 2. Panel par famille (format wide)

**Fichier**: `final_results/panel_grid_by_family.csv`

**Structure**:
- Lignes: (family_id, concept)
- Colonnes: annÃ©es
- IdÃ©al pour analyses longitudinales par famille

**Lecture**:

```python
import pandas as pd

# Charger le panel
panel = pd.read_csv('final_results/panel_grid_by_family.csv')

# Extraire une famille spÃ©cifique
family_100001 = panel[panel['family_id'] == '100001']

# Pivoter pour analyse
pivot = family_100001.set_index('concept').drop(columns=['family_id'])

# AccÃ©der Ã  une variable spÃ©cifique pour toutes les familles
ira_evolution = panel[panel['concept'] == 'ira_balance'].set_index('family_id')
```

### 3. Panel long individuel

**Fichier**: `final_results/panel_parent_child.csv`

**Structure**:
- Format long classique (panel data)
- Lignes: observations (personne Ã— annÃ©e)
- Colonnes: year, family_id, person_id, mother_id, father_id, variables...

**Lecture**:

```python
import pandas as pd

# Charger le panel long
panel_long = pd.read_csv('final_results/panel_parent_child.csv')

# Statistiques par annÃ©e
stats_by_year = panel_long.groupby('year').agg({
    'ira_balance': ['mean', 'median', 'std'],
    'num_children': ['mean', 'sum']
})

# Filtrer les parents uniquement
parents = panel_long[
    panel_long['person_id'].isin(panel_long['mother_id']) |
    panel_long['person_id'].isin(panel_long['father_id'])
]

# Panel par famille
family_panel = panel_long.groupby(['family_id', 'year']).first()
```

---

## âš¡ Optimisations mÃ©moire

Le pipeline implÃ©mente plusieurs stratÃ©gies d'optimisation pour gÃ©rer des datasets volumineux:

### 1. Downcast automatique des types

```python
# Avant optimisation
df['year'] = df['year'].astype('int64')     # 8 bytes par valeur
df['value'] = df['value'].astype('float64') # 8 bytes par valeur

# AprÃ¨s optimisation
df['year'] = df['year'].astype('int32')     # 4 bytes par valeur (-50%)
df['value'] = df['value'].astype('float32') # 4 bytes par valeur (-50%)
```

**Gain typique**: 40-60% de rÃ©duction de mÃ©moire

### 2. Types catÃ©goriels pour variables rÃ©pÃ©titives

```python
# Avant
df['module'] = df['module'].astype('string')  # ~6 bytes Ã— nb_rows

# AprÃ¨s
df['module'] = df['module'].astype('category')  # ~1 byte Ã— nb_rows + dict
```

**Gain**: 80-90% pour colonnes avec peu de valeurs uniques

### 3. Nullable integers (Int32 vs int32)

```python
# Utilisation de Int32 (nullable) au lieu de float pour prÃ©server les NaN
df['count'] = df['count'].astype('Int32')
```

**Avantages**:
- Conserve les valeurs manquantes sans conversion en float
- Ã‰conomie de mÃ©moire vs float64

### 4. Copy-on-write

```python
# ActivÃ© globalement
pd.options.mode.copy_on_write = True
```

**Avantages**:
- Ã‰vite les copies implicites
- RÃ©duit les pics de mÃ©moire

### 5. Traitement par chunks

```python
# Au lieu de tout charger en mÃ©moire
for chunk in pd.read_csv('huge_file.csv', chunksize=10000):
    process(chunk)
    write_to_parquet(chunk)
    del chunk
    gc.collect()  # LibÃ©ration explicite
```

### 6. Colonnes sÃ©lectives (usecols)

```python
# Ne lire que les colonnes nÃ©cessaires
df = pd.read_csv('data.csv', usecols=['col1', 'col2', 'col3'])
```

**Gain**: Proportionnel au ratio colonnes utilisÃ©es / colonnes totales

### RÃ©sumÃ© des gains

| Technique | Gain mÃ©moire typique |
|-----------|---------------------|
| Downcast int64â†’Int32 | 50% |
| Downcast float64â†’float32 | 50% |
| Stringâ†’Category (module, year) | 85% |
| Copy-on-write | 20-30% |
| Usecols (50% des colonnes) | 50% |
| **TOTAL CUMULÃ‰** | **70-85%** |

**Exemple rÃ©aliste**:
- Dataset brut: 8 GB en mÃ©moire
- AprÃ¨s optimisations: 1.2 - 2.4 GB
- Panel Parquet avec compression ZSTD: 300-500 MB sur disque

---

## ğŸ› ï¸ DÃ©pannage

### ProblÃ¨mes courants et solutions

#### 1. **Erreur: "No module named 'pyarrow'"**

**Cause**: DÃ©pendance Parquet manquante

**Solution**:
```bash
pip install pyarrow
# Ou alternative
pip install fastparquet
```

#### 2. **MemoryError lors de l'exÃ©cution**

**Cause**: Dataset trop volumineux pour la RAM disponible

**Solutions**:

A. RÃ©duire le nombre d'annÃ©es:
```bash
# Ã‰diter file_list.txt pour ne garder que quelques annÃ©es
python sas_to_csv.py --file-list file_list.txt --out-dir sorted_data
```

B. Augmenter le chunksize:
```python
# Dans build_final_panel.py, modifier:
chunksize = 5000  # Au lieu de 10000
```

C. Utiliser swap/virtual memory (Linux):
```bash
# CrÃ©er un fichier de swap de 8 GB
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

#### 3. **FileNotFoundError: final_grid.csv**

**Cause**: Ã‰tape prÃ©cÃ©dente non exÃ©cutÃ©e ou Ã©chouÃ©e

**Solution**: Relancer le pipeline complet
```bash
./run_all.sh
```

Ou manuellement les Ã©tapes manquantes:
```bash
python psid_tool.py --mapping out/mapping_long.csv --out-dir out
python merge_grid.py --file out/canonical_grid.csv --out out/canonical_grid_merged.csv < merge_groups.txt
cp out/canonical_grid_merged.csv out/final_grid.csv
```

#### 4. **Colonnes vides dans panel.parquet**

**Cause**: Variables pas prÃ©sentes dans les fichiers sources

**Diagnostic**:
```bash
# VÃ©rifier mapping_long.csv
cat out/mapping_long.csv | grep "variable_name"

# VÃ©rifier canonical_grid.csv
cat out/canonical_grid.csv | grep "variable_name"
```

**Solution**: VÃ©rifier que les fichiers sources contiennent bien ces variables

#### 5. **Erreur "cannot concatenate object of type"**

**Cause**: IncohÃ©rence de types entre chunks

**Solution**: Forcer dtype='string' partout
```python
# Dans le script, ajouter:
df = pd.read_csv(path, dtype='string', low_memory=False)
```

#### 6. **Temps d'exÃ©cution trÃ¨s long**

**Causes possibles**:
- Trop de fichiers Ã  convertir
- Pas de barres de progression (tqdm)
- Disque lent

**Solutions**:

A. Installer tqdm:
```bash
pip install tqdm
```

B. ParallÃ©liser manuellement:
```bash
# Convertir FAM et WLTH en parallÃ¨le dans 2 terminaux
# Terminal 1
python sas_to_csv.py --pattern "FAM*" --out-dir sorted_data

# Terminal 2
python sas_to_csv.py --pattern "WLTH*" --out-dir sorted_data
```

C. Utiliser un SSD si possible

#### 7. **Encoding errors lors de la lecture**

**Cause**: CaractÃ¨res spÃ©ciaux dans fichiers SAS/TXT

**Solution**:
```python
# Forcer l'encoding
df = pd.read_fwf(path, encoding='latin-1')  # ou 'cp1252'
```

#### 8. **Permission denied lors de l'exÃ©cution de run_all.sh**

**Cause**: Fichier pas exÃ©cutable

**Solution**:
```bash
chmod +x run_all.sh
./run_all.sh
```

---

## ğŸ’¡ Exemples d'utilisation

### Exemple 1: Analyse de l'Ã©volution du patrimoine

```python
import pandas as pd
import matplotlib.pyplot as plt

# Charger les donnÃ©es de patrimoine (WLTH)
df = pd.read_parquet(
    'final_results/panel.parquet',
    filters=[('module', '==', 'WLTH')],
    columns=['year', 'family_id', 'ira_balance', 'wealth_wo_equity']
)

# Calculer le patrimoine moyen par annÃ©e
wealth_by_year = df.groupby('year').agg({
    'ira_balance': 'mean',
    'wealth_wo_equity': 'mean'
}).reset_index()

# Visualisation
fig, ax = plt.subplots(figsize=(12, 6))
ax.plot(wealth_by_year['year'], wealth_by_year['ira_balance'],
        marker='o', label='IRA moyen')
ax.plot(wealth_by_year['year'], wealth_by_year['wealth_wo_equity'],
        marker='s', label='Patrimoine (sans equity)')
ax.set_xlabel('AnnÃ©e')
ax.set_ylabel('Montant ($)')
ax.set_title('Ã‰volution du patrimoine moyen 1999-2023')
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('wealth_evolution.png', dpi=300)
```

### Exemple 2: Comparaison familles avec/sans enfants

```python
import pandas as pd
import numpy as np

# Charger le panel parent-enfant
panel = pd.read_csv('final_results/panel_parent_child.csv')

# Identifier familles avec enfants
families_with_kids = panel[
    panel['mother_id'].notna() | panel['father_id'].notna()
]['family_id'].unique()

# CrÃ©er le flag
panel['has_children'] = panel['family_id'].isin(families_with_kids)

# Statistiques comparatives
comparison = panel.groupby(['year', 'has_children']).agg({
    'ira_balance': ['mean', 'median'],
    'num_children': 'mean',
    'family_id': 'nunique'
}).round(2)

print(comparison)
```

### Exemple 3: Tracking longitudinal d'une famille

```python
import pandas as pd

# Charger la grille par famille
panel_wide = pd.read_csv('final_results/panel_grid_by_family.csv')

# Extraire une famille spÃ©cifique
family_id = '100001'
family_data = panel_wide[panel_wide['family_id'] == family_id]

# Pivoter pour avoir concepts en index, annÃ©es en colonnes
family_pivot = family_data.set_index('concept').drop(columns=['family_id'])

# Afficher l'Ã©volution
print(f"Ã‰volution de la famille {family_id}:")
print(family_pivot.T)  # Transposer pour annÃ©es en lignes

# Calculer des taux de croissance
numeric_vars = ['ira_balance', 'wealth_wo_equity']
for var in numeric_vars:
    if var in family_pivot.index:
        series = pd.to_numeric(family_pivot.loc[var], errors='coerce')
        growth = series.pct_change() * 100
        print(f"\nCroissance annuelle {var}:")
        print(growth.dropna().round(2))
```

### Exemple 4: Export pour Stata/R

```python
import pandas as pd

# Charger panel long
df = pd.read_csv('final_results/panel_parent_child.csv')

# Export Stata
df.to_stata('panel_for_stata.dta', write_index=False, version=118)

# Export R (RDS)
import pyreadr
pyreadr.write_rds('panel_for_r.rds', df)

# Export CSV optimisÃ©
df.to_csv('panel_optimized.csv', index=False, compression='gzip')
```

### Exemple 5: RequÃªtes complexes sur Parquet

```python
import pandas as pd
import pyarrow.parquet as pq

# Ouvrir le dataset Parquet
dataset = pq.ParquetDataset('final_results/panel.parquet')

# RequÃªte complexe avec filtres multiples
table = dataset.read(
    columns=['year', 'family_id', 'ira_balance', 'num_children'],
    filters=[
        ('year', '>=', 2009),
        ('year', '<=', 2019),
        ('module', '==', 'FAM')
    ]
)

# Convertir en pandas
df = table.to_pandas()

# Analyse
summary = df.groupby('year').agg({
    'ira_balance': ['mean', 'median', 'std', 'count'],
    'num_children': 'mean',
    'family_id': 'nunique'
})

print(summary)
```

### Exemple 6: DÃ©tection de valeurs aberrantes

```python
import pandas as pd
import numpy as np

# Charger donnÃ©es
df = pd.read_parquet(
    'final_results/panel.parquet',
    columns=['year', 'family_id', 'ira_balance']
)

# Convertir en numÃ©rique
df['ira_balance'] = pd.to_numeric(df['ira_balance'], errors='coerce')

# Calculer quartiles et IQR
Q1 = df['ira_balance'].quantile(0.25)
Q3 = df['ira_balance'].quantile(0.75)
IQR = Q3 - Q1

# DÃ©tecter outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = df[
    (df['ira_balance'] < lower_bound) |
    (df['ira_balance'] > upper_bound)
]

print(f"Nombre d'outliers: {len(outliers)}")
print(f"% d'outliers: {len(outliers)/len(df)*100:.2f}%")
print("\nExemples d'outliers:")
print(outliers.head(10))
```

---

## ğŸ“Š SchÃ©ma de donnÃ©es complet

### Relations entre tables

```
mapping_long.csv (dictionnaire)
    â”‚
    â”œâ”€â†’ canonical_grid.csv (pivot)
    â”‚       â”‚
    â”‚       â””â”€â†’ final_grid.csv (aprÃ¨s fusion)
    â”‚               â”‚
    â”‚               â”œâ”€â†’ panel.parquet/ (donnÃ©es optimisÃ©es)
    â”‚               â”‚
    â”‚               â””â”€â†’ panel_grid_by_family.csv (wide par famille)
    â”‚
    â””â”€â†’ codes_resolved_audit.csv (audit)

sorted_data/*_full.csv (sources)
    â”‚
    â””â”€â†’ fam_wlth_inventory.csv (inventaire)
```

### CardinalitÃ©s

- **mapping_long.csv**: ~50,000 - 200,000 lignes (dÃ©pend du nb d'annÃ©es Ã— variables)
- **canonical_grid.csv**: ~500 - 2,000 lignes (concepts uniques)
- **panel.parquet/**: 1M - 50M lignes (dÃ©pend des annÃ©es et familles)
- **panel_grid_by_family.csv**: ~10,000 - 500,000 lignes (familles Ã— concepts)

---

## ğŸ” ConsidÃ©rations de confidentialitÃ©

Les donnÃ©es PSID peuvent contenir des informations sensibles. Bonnes pratiques:

1. **Ne jamais commiter les donnÃ©es** dans un repo Git
   ```bash
   # Ajouter Ã  .gitignore
   sorted_data/
   out/
   final_results/
   *.csv
   *.parquet
   ```

2. **Chiffrer les donnÃ©es au repos** (recommandÃ©)
   ```bash
   # Exemple avec GPG
   tar czf - final_results/ | gpg -c > final_results.tar.gz.gpg
   ```

3. **ContrÃ´ler l'accÃ¨s** aux fichiers
   ```bash
   chmod 600 final_results/*.csv
   chmod 700 final_results/panel.parquet/
   ```

---

## ğŸ“š Ressources PSID

- **Site officiel**: https://psidonline.isr.umich.edu/
- **Documentation des variables**: https://simba.isr.umich.edu/default.aspx
- **User Guide**: https://psidonline.isr.umich.edu/Guide/default.aspx
- **FAQs**: https://psidonline.isr.umich.edu/FAQ/

---

## ğŸ¤ Contribution

Pour signaler un bug ou proposer une amÃ©lioration:

1. VÃ©rifier que le problÃ¨me n'existe pas dÃ©jÃ 
2. CrÃ©er une issue avec:
   - Description du problÃ¨me/amÃ©lioration
   - Ã‰tapes de reproduction (si bug)
   - Logs d'erreur
   - Version Python et dÃ©pendances

---

## ğŸ“ Licence

Ce projet est un outil de recherche acadÃ©mique. Les donnÃ©es PSID sont soumises Ã  leur propre licence d'utilisation.

---

## âœ¨ Changelog

### Version actuelle (2024)

**Nouvelles fonctionnalitÃ©s:**
- Pipeline complet automatisÃ© via `run_all.sh`
- Support Parquet avec compression ZSTD
- Optimisations mÃ©moire agressives (Int32, float32, category)
- Logging colorisÃ© avec timestamps
- Support barres de progression (tqdm)
- Mode rebuild pour panel.parquet

**Scripts principaux:**
- `sas_to_csv.py`: Conversion SAS/TXT optimisÃ©e
- `create_mapping.py`: Mapping avec normalisation avancÃ©e
- `psid_tool.py`: Grille canonique avec rÃ©solution de conflits
- `merge_grid.py`: Fusion de lignes configurables
- `build_final_panel.py`: Panel Parquet memory-efficient
- `build_panel_parent_child.py`: Panel famille-enfant avec relations

**AmÃ©liorations:**
- Gestion robuste des erreurs
- Documentation exhaustive
- Copy-on-write pour rÃ©duction mÃ©moire
- Partitionnement intelligent par annÃ©e/module

---

## ğŸ“ Support

Pour toute question technique:
1. Consulter la section [DÃ©pannage](#dÃ©pannage)
2. VÃ©rifier les [Exemples d'utilisation](#exemples-dutilisation)
3. Lire les docstrings des scripts (en-tÃªte de chaque .py)

---

**DerniÃ¨re mise Ã  jour**: 2024-11-01
**Version**: 3.0
**Auteur**: Pipeline PSID RA Team
